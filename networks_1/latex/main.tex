\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{babel}
\usepackage{amsmath, amssymb}

\usepackage{enumitem}

\usepackage{graphicx}

\usepackage{listings}

\title{Networks problem sheet 1}
\author{Aleksander Kaminski}
\date{\today}

\bibliographystyle{ieeetr}


\pdfsuppresswarningpagegroup=1

\begin{document}

\maketitle

\lstset{language=Python, keywordstyle={\bfseries \colour{blue}}}

\section{Introduction}


\begin{enumerate}[label={(1.\alph*):}]
    \item
        I've already got some programming experience in matlab which guided my decision to never use it again. I will choose to use python, as I don't know it yet.
    \item
        I primarily used website 'learnxiny' \footnote{https://learnxinyminutes.com/docs/python/}to learn the syntax. This was enough for me as a reference to get started. Any other problems I had I consulted the python docs.\footnote{https://docs.python.org/3/c-api/set.html}  
        \setcounter{enumi}{3}
    \item
        The format (gml) seems to simply be a text file, firstly defining the nodes and their attributes (label, id) and then a definition of the edges, with edge attributes. The power network didn't have any labels, so I had to manually tell \lstinline{networkx.read_gml} to take the id as labels. I used the US Power grid dataset\cite{power}, and the Les Mis√©rables\cite{lesmis} dataset. I shall call these power and lesmis, respectively.
    \item
        Firstly, the lesmis dataset, I generated a graph visualisation centered on the node with the highest degree, as shown in Fig. \ref{fig:lesmis_gr}. As expected, this node represented the main character of the book. Fig. \ref{fig:lesmis_deg} shows that the second-highest degree nodes had significantly fewer connections than the highest, further showing that the highest-degree character appears with a much more varied audience. \\
 
        \begin{figure}
            \includegraphics[width=\linewidth]{./LesMis_ego_vis.png}
            \caption{lesmis ego graph}
            \label{fig:lesmis_gr}
        \end{figure}
        
        \begin{figure}
            \includegraphics[width=\linewidth]{./LesMis_degree_dist.png}
            \caption{lesmis degree distribution}
            \label{fig:lesmis_deg}
        \end{figure}

        For the power dataset, I firstly used python and networkx to generate a csv file with the degree distribution, then plotted it using R with ggplot2. I also used R to analyze the data. Firstly, I plotted the log-log graphs as shown in Fig. \ref{fig:power_loglog}. This fit wasn't linear enough to deem correct, and so I moved on to a log- plot as in Fig. \ref{fig:power_log}, which clearly showed a linear behaviour in the subset of degrees [2, 14]. From this, I extracted the exponential behaviour as $\exp(8.25-0.528* degree) $. Fig. \ref{fig:power_vis} shows that there are a large number of "leaf" nodes, namely, nodes which only connect to one other node.  

        \begin{figure}
            \includegraphics[width=\linewidth]{./power_degree_dist.png}
            \caption{power degree distribution}
            \label{fig:power_deg}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{./power_deg_dist_loglog.png}
            \caption{power degree distribution}
            \label{fig:power_loglog}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{./power_deg_dist_log.png}
            \caption{power degree distribution}
            \label{fig:power_log}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{./power_vis.png}
            \caption{power network visualization}
            \label{fig:power_vis}
        \end{figure}

\end{enumerate}

\pagebreak

\section{Mathematical Toolbox}

\begin{enumerate}[label={(2. \alph*)}]
    \item
        Fig. \ref{fig:bern} shows that the computed variance matches the mathematics. The mean was computed to be $p$. 
        \begin{figure}
            \includegraphics[width=\linewidth]{./bernoulli_var.png}
            \caption{bernoulli process variance, with overlayed $p(1-p)$:}
            \label{fig:bern}
        \end{figure}
    \item
        I simulated 10000 random walkers and plotted the resulting position distribution for 5000, 10000 and 15000 samples. From theory (the notes), we know the data should follow the gaussian profile \[
        p(x;t) = \frac{1}{\sqrt{2 \pi D t}} \exp{\frac{-x^2}{4 D t}} 
        .\] With D given by,
        \[
            D = \frac{\langle x^2 \rangle}{2} = \frac{1}{2}
        .\] With this, the data didn't fit, as the gaussian is not normalized properly. The correct normalisation should be, for continuous data $\frac{1}{\sqrt{4\pi D t}}$. (Is this a problem with the notes?) With this normalisation, our gaussian is at half of the amplitude of the data, and that's because at any given iteration, we can only access either the even-, or odd-numbered positions. Thus, we are overcompensating and need to multiply by two. Thus, for our data, the correct normalisation is 
        \[
            p(x;t) = \frac{1}{\sqrt{\pi D t}} \exp{\frac{-x^2}{4 D t}} 
        .\]   
        Figs. \ref{fig:rw5000}, \ref{fig:rw10000}, \ref{fig:rw15000} show the data fitting to the modified gaussian correcly. 

        \begin{figure}
            \includegraphics[width=\linewidth]{./random_walk5000.png}
            \caption{random walk after 5000 iterations}
            \label{fig:rw5000}
        \end{figure}
        \begin{figure}
            \includegraphics[width=\linewidth]{./random_walk10000.png}
            \caption{random walk after 10000 iterations}
            \label{fig:rw10000}
        \end{figure}
        \begin{figure} 
            \includegraphics[width=\linewidth]{./random_walk15000.png}
            \caption{random walk after 15000 iterations}
            \label{fig:rw15000}
        \end{figure}
    \item
        Generating the matrices I found the distribution shown in Figs. \ref{fig:eig_dist_small}, \ref{fig:eig_dist_big}. The distribution looks like a semi-circle.
        \begin{figure}
            \includegraphics[width=\linewidth]{./eig_dist_small.png}
            \caption{Eigenvalue distribution for bin < 100}
            \label{fig:eig_dist_small}
        \end{figure}
        \begin{figure}
            \includegraphics[width=\linewidth]{./eig_dist_big.png}
            \caption{Eigenvalue distribution for bin > 100}
            \label{fig:eig_dist_big}
        \end{figure}

\end{enumerate}

\pagebreak

\section{Connectedness} 

\begin{enumerate}[label={(3. \alph*)}]
    \item
        Not strongly connected implies that for some node i, there exists a node that cannot be reached by a walk. Thus, there will be a set of nodes with an asymptotic density of walkers equal to zero. \\As an example, consider a strongly connected graph, g, with the addition of one directed node pointing into a node of g. Random walkers starting inside g will be contained there, and any walkers starting on the node outside g will soon end up inside. Thus, g will act as a sort of trap for walkers.
    \item
        The difference between directed and undirected graphs is in the symmetry of the adjacency matrix. We know from liner algebra that hermitian matrices (and thus, more weakly, symmetric matrices) have real eigenvalues. Thus, we would expect the eigenvalues of an undirected graph to generally be complex.
\end{enumerate}

\pagebreak

\section{Clustering Coefficients}

\begin{enumerate}[label={(4. \alph*)}]    
     \item 
         I generated a network of size 10 given by the adjacency matrix
\begin{equation*}
    A = 
    \begin{pmatrix} 
        0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
        1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 1\\
        1 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\
        0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
        1 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0\\
        0 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 0 & 0\\
        0 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0\\
        0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 0\\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1\\
        0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0
    \end{pmatrix} 
\end{equation*}
    


    This network can be seen in Fig. \ref{fig:small_clustering}, where the node labels are the local clustering coefficient. For this network, the global clustering coefficient is 0.473. 


    \begin{figure}[h]
        \centerline{\includegraphics[width=0.8\linewidth]{./small_clustering.png}}
        \caption{Random network with varying local clustering coefficients}
        \label{fig:small_clustering}
    \end{figure}

\end{enumerate}

\pagebreak

\section{Small World}

\begin{enumerate}[label={(5. \alph*)}]
    \item 
        Firstly, for a network of size $N$, with connectivity, $k$, the maximum number of shortcuts is   
        \[
            S_{max} = \frac{(N - k)(N - k - 1)}{2} - \frac{k(k+1)}{2}
        .\] 
        which can be seen by calculating the number of zeros in the adjacency matrix. \\ Then, for $N = 20$ and $k=3$, I calculated diameters and clustering coefficient averaged over 10 runs. The data can be seen in Fig. \ref{fig:small_world}. The diameter plateaus at integer values, which is to be expected. Naively, I would predict the length of each plateau to be roughly $N$, but this is not the case here for D=2. 
    
    \begin{figure}[h]
        \centerline{\includegraphics[width=1.3\linewidth]{./small_world.png}}
        \caption{Clustering coefficient and diameter of a lattice-network with 20 nodes, and 6 neighbors}
        \label{fig:small_world}
    \end{figure}
\end{enumerate}

\pagebreak

\section{Centrality measures}

\begin{enumerate}[label={(6. \alph*)}]
    \item I generated undirected networks with $N=20$ with varying densities, $p$, defined as the ratio of off-diagonal 1s to zeroes of the adjacency matrix. For each such network, I calculated the distributions of centrality measures:
        \begin{enumerate}
            \item Degree centrality
            \item Betweenness Centrality
            \item Closeness Centrality
            \item Katz Centrality\footnote{I used an alpha parameter of 0.01 to assure convergence}
        \end{enumerate}
        For each pair of centralities, I then calculated the Pearson correlation. The data is shown in Fig. \ref{fig:centr}. Correlations peak at $p=0$ and  $p=1$, as here every node will have the same value of centrality. Only at the beginning, with very low density of connections, are the centralities correlated. All of the centralities are highly uncorrelated with Katz centrality, for most densities.  
     \begin{figure}[h]
        \centerline{\includegraphics[width=0.9\linewidth]{./undirected_centrality.png}}
        \caption{Centrality Correlations for $N=20$ graphs}
        \label{fig:centr}
    \end{figure}


    \item Assuming the same notation as the paper, with $Au = \lambda u$ and $\nu^T A = \nu^T \lambda$, and differences  $A \mapsto A + \Delta A$, $\lambda \mapsto \lambda + \Delta \lambda$, $\nu^T \mapsto \nu^T + \Delta \nu^T$, $u \mapsto u + \Delta u$. Then, ignoring second order terms and assuming all differences are small,
    \begin{equation*}
        \begin{align} 
            \nu^T(A + \Delta A)(u + \Delta u) &= \nu^T (\lambda + \Delta \lambda)(u + \Delta u) .\\
            \nu^T(Au + A\Delta u + \Delta A u) &= \nu^T (\lambda u + \lambda \Delta u + \Delta \lambda u).\\
            \nu^T\lambda\Delta u + \nu^T\Delta Au &= \nu^T \lambda \Delta u + \nu^T \Delta \lambda u.\\
            \Delta \lambda &= \frac{\nu^T \Delta A u}{\nu^T u}. 
        \end{align}
    \end{equation*}

    For the removal of a single edge $i \rightarrow j$, we need only remove the corresponding entry in the matrix. So, $(\Delta A)_{ij} = -A_{lm} \delta_{il}\delta_{jm}$ and thus, we get that
    \[
        \hat{I}_{ij} = -\frac{\Delta \lambda_{ij}}{\lambda} = \frac{A_{ij}\nu_{i}u_{j}}{\nu^T u}
    .\] 

    For the case of removal of a whole node $i$, we cannot assume that  $\Delta u$ is small, as the new  $u$ will have a 0 at  $i$ position. We can set  $\Delta u = \delta u - u_i\hat{e}_i$, where $\delta u_k$ are the small variations in the $k \neq i$ components. Removing a node removes the whole row and column from the matrix, and so the perturbation is given by $(\Delta A)_{kl} = -A_{kl}(\delta_{ki} + \delta_{li})$. Substituting $\Delta u$, the dynamical importance of a node is given by,
    \[
        \hat{I}_{i} = -\frac{\Delta \lambda_{i}}{\lambda} = -\frac{\nu^T\Delta A u - u_i \nu^T \Delta A \hat{e}_i}{\nu^Tu - \nu_i u_i}
    .\] 
    Now using the expression for $\Delta A$ we can see that  $\nu^T A u = -2 \lambda u_i \nu_i$ and  $u_i \nu^T \Delta A \hat{e}_i = \lambda u_i \nu_i$. Also, with a high enough network size, the denominator can be approximated as $\nu^T u - \nu_i u_i \approx \nu^T u$. Thus, we end up with
    \[
        \hat{I}_{i} = \frac{\nu_i u_i}{\nu^T u}
    .\]



\end{enumerate}

\pagebreak

\bibliography{citations}

\end{document}
